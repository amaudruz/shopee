{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Images\" data-toc-modified-id=\"Images-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Images</a></span></li><li><span><a href=\"#Texts\" data-toc-modified-id=\"Texts-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Texts</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "from functools import partial\n",
    "import PIL\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "import timm\n",
    "np.random.seed(1337)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxDS(Dataset):\n",
    "    def __init__(self, data, images_path, return_triplet = True):\n",
    "        super().__init__()\n",
    "        self.imgs = data['image'].tolist()\n",
    "        self.unique_labels = data['label_group'].unique().tolist()\n",
    "        self.labels = data['label_group'].astype('category')\n",
    "        self.label_codes = self.labels.cat.codes\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img = self._get_item(idx)\n",
    "        label = self.label_codes.iloc[idx]\n",
    "        return img, label\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def _get_item(self, idx):\n",
    "        im = PIL.Image.open(os.path.join(self.images_path, self.imgs[idx]))\n",
    "        im = torch.tensor(np.array(im) / 255.0, dtype = torch.float).permute(2,0,1)\n",
    "        return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "small_images_dir = 'data/small_train_images/'\n",
    "n_classes = df['label_group'].nunique()\n",
    "\n",
    "labels = np.random.permutation(df['label_group'].unique())\n",
    "\n",
    "train_perc = 0.7\n",
    "train_idx = int(train_perc * len(labels))\n",
    "\n",
    "train_labels = labels[:train_idx]\n",
    "val_labels = labels[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df[df['label_group'].isin(train_labels)]\n",
    "val_df = df[df['label_group'].isin(val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "\n",
    "vision_model = 'resnet50'\n",
    "\n",
    "bs = 64\n",
    "tr_ds = SoftMaxDS(train_df, small_images_dir)\n",
    "tr_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, pin_memory = True)\n",
    "\n",
    "val_ds = SoftMaxDS(val_df, small_images_dir)\n",
    "val_dl = DataLoader(val_ds, batch_size = bs, shuffle = False, pin_memory = True)\n",
    "device = torch.device('cuda')\n",
    "\n",
    "val_label_count = val_df['label_group'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMBRes(nn.Module) :\n",
    "    def __init__(self, pretrained_image_embedor='resnet50',\n",
    "                output_dim=512) :\n",
    "        super(EMBRes, self).__init__()\n",
    "        self.image_embedor = timm.create_model(pretrained_image_embedor, pretrained=True)\n",
    "        self.image_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.head = nn.Sequential(nn.Linear(2048, output_dim), \n",
    "                                  #nn.ReLU(),\n",
    "                                  )\n",
    "        \n",
    "        for m in self.head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                sz = m.weight.data.size(-1)\n",
    "                m.weight.data.normal_(mean=0.0, std=1/np.sqrt(sz))\n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                m.bias.data.zero_()\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "    \n",
    "    def _get_embs(self, x) :\n",
    "        images = x\n",
    "        out_images = self.image_embedor.forward_features(images)\n",
    "        out_images = self.image_pool(out_images).squeeze()\n",
    "        #return F.normalize(out_images, dim=-1)\n",
    "        return out_images\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        out_images = self._get_embs(x)\n",
    "        \n",
    "        return self.head(out_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ronghuaiyang/arcface-pytorch\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Embeddings normalization is not done in the model but in the arcface metric***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EMBRes(vision_model).to(device)\n",
    "metric_fc = ArcMarginProduct(512, df['label_group'].nunique(), s=30, m=0.5, easy_margin=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ColorJitter(.3,.3,.3),\n",
    "                                       transforms.RandomRotation(5),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       normalize\n",
    "                                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([normalize\n",
    "                                     ])\n",
    "\n",
    "n_epochs = 30\n",
    "\n",
    "lf = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-2\n",
    "wd = 0\n",
    "no_decay = [\"bias\", \"BatchNorm2d.weight\", \"BatchNorm2d.bias\", \"LayerNorm.weight\", 'LayerNorm.bias',\n",
    "            \"BatchNorm1d.weight\", \"BatchNorm1d.bias\"]\n",
    "\n",
    "params = list(model.named_parameters()) + list(metric_fc.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in params if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": wd,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in  model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr =lr, pct_start = 0.3, #anneal_strategy = 'linear',\n",
    "                                            total_steps = int(n_epochs * len(tr_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1(embeddings, ls, threshold) :\n",
    "    dists = torch.cdist(embeddings, embeddings)\n",
    "    \n",
    "    print('Computing distances...')\n",
    "    distances, indices = torch.topk(dists, 50, dim=1, largest=False)\n",
    "    \n",
    "    print('Making predictions...')\n",
    "    THRESHOLD = threshold\n",
    "    preds = [[] for _ in range(embeddings.shape[0])]\n",
    "    for i in tqdm(range(distances.shape[0])) :\n",
    "        IDX = torch.where(distances[i,]<THRESHOLD)[0]\n",
    "        IDS = indices[i,IDX]\n",
    "        preds[i] = IDS.cpu().numpy()\n",
    "            \n",
    "    print('Computing f1 score...')\n",
    "    label_counts = ls.value_counts()\n",
    "    f_score = 0 \n",
    "    for i in tqdm(range(embeddings.shape[0])) :\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        true_label = ls.iloc[i]\n",
    "        for index in preds[i] :\n",
    "            if ls.iloc[index] == true_label :\n",
    "                tp += 1\n",
    "            else :\n",
    "                fp += 1\n",
    "        fn = label_counts[true_label] - tp\n",
    "        #print(label_counts[true_label]-1, tp)\n",
    "        f_score += 2*tp / (label_counts[true_label] + len(preds[i]))\n",
    "    f_score = f_score/embeddings.shape[0]\n",
    "    print('f1 score : {}'.format(f_score))\n",
    "    return f_score, preds, distances, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "435819e2a8af4d629cfe5871c17678af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cdf46274e2146a3a2554bf7d6b62148",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tr_losses = []\n",
    "val_scores = []\n",
    "thr = 0.6\n",
    "prev_best_f_score = -100000\n",
    "for ep in tqdm(range(n_epochs)):\n",
    "    \n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    pbar = tqdm(tr_dl)\n",
    "    for imgs, labels in pbar:\n",
    "        \n",
    "        imgs = train_transforms(imgs.to(device))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        feature = model(imgs)\n",
    "        labels = labels.long().to(device)\n",
    "        out = metric_fc(feature, labels)\n",
    "        loss = lf(out, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "        \n",
    "        tr_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {round(np.mean(tr_loss),3)}\")\n",
    "    \n",
    "    if ep%2==0 :\n",
    "        torch.save(model.state_dict(), 'data/tests_model_image/model_arcf_split_res50_ep_{}.pth'.format(ep))\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_dl)\n",
    "        embs = []\n",
    "        for imgs, _ in pbar:\n",
    "            imgs = val_transforms(imgs.to(device))\n",
    "            feature = model(imgs)\n",
    "            embs.append(feature)\n",
    "        embs = F.normalize(torch.cat(embs, 0))\n",
    "    \n",
    "    f_score = compute_f1(embs, val_df['label_group'], thr)[0]\n",
    "    if f_score > prev_best_f_score :\n",
    "        prev_best_f_score = f_score\n",
    "        torch.save(model.state_dict(), 'data/tests_model_image/model_arcf_split_res50_best.pth'.format(ep))\n",
    "        print('Saved best model ep {} with f score : {} for thresh : {}'.format(ep, vl, thr))\n",
    "    \n",
    "    \n",
    "    tr_losses.append(tr_loss)\n",
    "    val_scores.append(f_score)\n",
    "    summary = f\"Ep {ep}: Train loss {np.asarray(tr_loss).mean()} | Val f score {f_score}\"\n",
    "    print(summary) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'data/tests_model_image/model_class_ep_30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('data/tests_model_image/model_class_ep_30.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ds = SoftMaxDS(df, small_images_dir)\n",
    "testing_dl = DataLoader(testing_ds, batch_size = bs, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ColorJitter(.3,.3,.3),\n",
    "                                       transforms.RandomRotation(5),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       normalize\n",
    "                                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([normalize\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865f9ae220c42cbb40ef954e3f250c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(testing_dl)\n",
    "    for image, labels in pbar:\n",
    "        x = val_transforms(image.to(device))\n",
    "        y = model(x)\n",
    "        embs.append(y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = F.normalize(torch.cat(embs,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df = pd.DataFrame(embs.numpy())\n",
    "emb_cols = [f'emb_{i}' for i in embs_df.columns]\n",
    "embs_df.columns = emb_cols\n",
    "embs_df.to_csv('data/tests_model_image/train_embs_arcf_30ep.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "from functools import partial\n",
    "import PIL\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxDS(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        super().__init__()\n",
    "        self.unique_labels = data['label_group'].unique().tolist()\n",
    "        self.labels = data['label_group'].tolist()\n",
    "        self.label_to_index_dict = (data.reset_index(drop = True)\n",
    "                                    .groupby('label_group')\n",
    "                                    .apply(lambda x: x.index.tolist())\n",
    "                                    .to_dict())\n",
    "        self.texts = tokenizer(data['title'].values.tolist(), return_tensors = 'pt',\n",
    "                               padding=True, truncation=True, max_length = 40)\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # anchor data\n",
    "        label = self.labels[idx]\n",
    "        txt = self._get_item(idx)\n",
    "        \n",
    "        return txt, label\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def _get_item(self, idx):\n",
    "        txt = {'input_ids' : self.texts['input_ids'][idx], \n",
    "               'attention_mask' : self.texts['attention_mask'][idx]}\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "small_images_dir = 'data/small_train_images/'\n",
    "n_classes = df['label_group'].nunique()\n",
    "np.random.seed(1337)\n",
    "\n",
    "# train val split\n",
    "\n",
    "train_perc = 0.7\n",
    "n_train_examples = int(train_perc * len(df))\n",
    "\n",
    "train_df = df.iloc[:n_train_examples]\n",
    "val_df = df.iloc[n_train_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "\n",
    "language_model = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(language_model)\n",
    "\n",
    "bs = 64\n",
    "tr_ds = SoftMaxDS(df, tokenizer)\n",
    "tr_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, pin_memory = True)\n",
    "\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EMBBert(nn.Module) :\n",
    "    def __init__(self, pretrained_text_embedor='bert-base-uncased',\n",
    "                output_dim=512) :\n",
    "        super(EMBBert, self).__init__()\n",
    "        self.text_embedor = BertModel.from_pretrained(pretrained_text_embedor)\n",
    "        self.head = nn.Sequential(nn.Linear(768, output_dim), \n",
    "                                  #nn.ReLU(), \n",
    "                                  #nn.Linear(1024, output_dim)\n",
    "                                 )\n",
    "        \n",
    "        for m in self.head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                sz = m.weight.data.size(-1)\n",
    "                m.weight.data.normal_(mean=0.0, std=1/np.sqrt(sz))\n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                m.bias.data.zero_()\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "    \n",
    "    def forward(self, x) :\n",
    "        texts = x\n",
    "        \n",
    "        out_text = self.text_embedor(texts['input_ids'], \n",
    "                                     attention_mask=texts['attention_mask'])[0][:,0,:]\n",
    "        return F.normalize(self.head(out_text), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/ronghuaiyang/arcface-pytorch\n",
    "\n",
    "class ArcMarginProduct(nn.Module):\n",
    "    r\"\"\"Implement of large margin arc distance: :\n",
    "        Args:\n",
    "            in_features: size of each input sample\n",
    "            out_features: size of each output sample\n",
    "            s: norm of input feature\n",
    "            m: margin\n",
    "            cos(theta + m)\n",
    "        \"\"\"\n",
    "    def __init__(self, in_features, out_features, s=30.0, m=0.50, easy_margin=False):\n",
    "        super(ArcMarginProduct, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.weight = Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "        self.easy_margin = easy_margin\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.th = math.cos(math.pi - m)\n",
    "        self.mm = math.sin(math.pi - m) * m\n",
    "\n",
    "    def forward(self, input, label):\n",
    "        # --------------------------- cos(theta) & phi(theta) ---------------------------\n",
    "        cosine = F.linear(F.normalize(input), F.normalize(self.weight))\n",
    "        sine = torch.sqrt((1.0 - torch.pow(cosine, 2)).clamp(0, 1))\n",
    "        phi = cosine * self.cos_m - sine * self.sin_m\n",
    "        if self.easy_margin:\n",
    "            phi = torch.where(cosine > 0, phi, cosine)\n",
    "        else:\n",
    "            phi = torch.where(cosine > self.th, phi, cosine - self.mm)\n",
    "        # --------------------------- convert label to one-hot ---------------------------\n",
    "        # one_hot = torch.zeros(cosine.size(), requires_grad=True, device='cuda')\n",
    "        one_hot = torch.zeros(cosine.size(), device='cuda')\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        # -------------torch.where(out_i = {x_i if condition_i else y_i) -------------\n",
    "        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)  # you can use torch.where if your torch.__version__ is 0.4\n",
    "        output *= self.s\n",
    "        # print(output)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Embeddings normalization is not done in the model but in the arcface metric***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EMBBert().to(device)\n",
    "metric_fc = ArcMarginProduct(512, df['label_group'].nunique(), s=30, m=0.5, easy_margin=False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_device(text, device):\n",
    "    return {'input_ids' : text['input_ids'].to(device),\n",
    "            'attention_mask' : text['attention_mask'].to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 30\n",
    "\n",
    "lf = nn.CrossEntropyLoss()\n",
    "\n",
    "lr = 1e-2\n",
    "wd = 0\n",
    "no_decay = [\"bias\", \"BatchNorm2d.weight\", \"BatchNorm2d.bias\", \"LayerNorm.weight\", 'LayerNorm.bias',\n",
    "            \"BatchNorm1d.weight\", \"BatchNorm1d.bias\"]\n",
    "\n",
    "params = list(model.named_parameters()) + list(metric_fc.named_parameters())\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in params if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": wd,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in  model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr =lr, pct_start = 0.3, #anneal_strategy = 'linear',\n",
    "                                            total_steps = int(n_epochs * len(tr_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = next(iter(tr_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01255c4e0dc46ac8c57ac3b52a772a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=30.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f5951cb0092491fa54f72272bba4077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-50c22404608c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtxts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtxts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtxts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-f4f7064a1f00>\u001b[0m in \u001b[0;36mtext_to_device\u001b[0;34m(text, device)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     return {'input_ids' : text['input_ids'].to(device),\n\u001b[0m\u001b[1;32m      3\u001b[0m             'attention_mask' : text['attention_mask'].to(device)}\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "tr_losses = []\n",
    "val_losses = []\n",
    "for ep in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    pbar = tqdm(tr_dl)\n",
    "    for txts in pbar:\n",
    "        \n",
    "        txts = text_to_device(txts, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        feature = model(txts)\n",
    "        labels = labels.long().to(device)\n",
    "        out = metric_fc(feature, labels)\n",
    "        loss = lf(out, labels)\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        sched.step()\n",
    "        \n",
    "        tr_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {round(np.mean(tr_loss),3)}\")\n",
    "    \n",
    "    if ep%2==0 :\n",
    "        torch.save(model.state_dict(), 'data/tests_model_image/model_arcfsplit_ep_{}.pth'.format(ep))\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_dl)\n",
    "        for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "\n",
    "            anchor = val_transforms(anchor_image.to(device)), text_to_device(anchor_text, device)\n",
    "            pos = val_transforms(pos_image.to(device)), text_to_device(pos_text, device)\n",
    "            neg = val_transforms(neg_image.to(device)), text_to_device(neg_text, device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                \n",
    "                anchor_emb = model(anchor)\n",
    "                pos_emb = model(pos)\n",
    "                neg_emb = model(neg)\n",
    "                loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            pbar.set_description(f\"Val loss: {round(np.mean(val_loss),3)}\")\n",
    "    tr_losses.append(tr_loss)\n",
    "    summary = f\"Ep {ep}: Train loss {np.asarray(tr_loss).mean()} | Val loss {np.asarray(val_loss).mean()}\"\n",
    "    print(summary) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-17-f4f7064a1f00>\u001b[0m(2)\u001b[0;36mtext_to_device\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      1 \u001b[0;31m\u001b[0;32mdef\u001b[0m \u001b[0mtext_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 2 \u001b[0;31m    return {'input_ids' : text['input_ids'].to(device),\n",
      "\u001b[0m\u001b[0;32m      3 \u001b[0;31m            'attention_mask' : text['attention_mask'].to(device)}\n",
      "\u001b[0m\n",
      "ipdb> text\n",
      "[{'input_ids': tensor([[  101, 14163,  7520,  ...,     0,     0,     0],\n",
      "        [  101,  9152,  3726,  ...,     0,     0,     0],\n",
      "        [  101, 18906,  5575,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [  101,  6090, 15444,  ...,     0,     0,     0],\n",
      "        [  101,  3681, 26348,  ...,     0,     0,     0],\n",
      "        [  101,  1031, 19337,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}, tensor([ 420878019, 3639207904,  302466871, 4245771399, 3157557152, 1322597928,\n",
      "          57219072,  159351600, 3047779462,  574325131, 1226500780, 3893832039,\n",
      "        2398547853, 1322749330, 1124556741,  584558550, 1238910730,  627560398,\n",
      "         154333354, 2235705163, 1397230201, 4088587398, 3494512671,  353320183,\n",
      "         875744553, 3969610832, 1163569239, 1370809083, 2049141115, 3388174522,\n",
      "         159351600, 1319225132, 2573756909,  784759211, 1126912250,  962477933,\n",
      "        1929387651, 1411980274,  899193257, 1468355072,  480632610, 3776555725,\n",
      "         378511024, 2714821158, 2463260986, 3984820994,  587285122,  848078453,\n",
      "         115650745, 2403374241,   73579052,  121602456, 4252879978, 1980845703,\n",
      "         431145971, 4237830533, 2457457611,   19273520, 1884996979, 2865003405,\n",
      "        1377712939, 2659286920, 1256302275, 3489985175])]\n",
      "ipdb> c\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'data/tests_model_image/model_class_ep_30.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('data/tests_model_image/model_class_ep_30.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ds = SoftMaxDS(df, small_images_dir)\n",
    "testing_dl = DataLoader(testing_ds, batch_size = bs, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ColorJitter(.3,.3,.3),\n",
    "                                       transforms.RandomRotation(5),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       normalize\n",
    "                                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([normalize\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6865f9ae220c42cbb40ef954e3f250c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(testing_dl)\n",
    "    for image, labels in pbar:\n",
    "        x = val_transforms(image.to(device))\n",
    "        y = model(x)\n",
    "        embs.append(y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = F.normalize(torch.cat(embs,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df = pd.DataFrame(embs.numpy())\n",
    "emb_cols = [f'emb_{i}' for i in embs_df.columns]\n",
    "embs_df.columns = emb_cols\n",
    "embs_df.to_csv('data/tests_model_image/train_embs_arcf_30ep.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
