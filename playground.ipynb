{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \n",
       "0                          Paper Bag Victoria Secret    249114794  \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 666, 7572, 6172, ..., 3388, 1968,  132], dtype=int16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label_group'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "posting_id     0\n",
       "image          0\n",
       "image_phash    0\n",
       "title          0\n",
       "label_group    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11014"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['label_group'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of offers per product (offer = (image + text description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f919c4bbed0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAQE0lEQVR4nO3df6zddX3H8edrrfgDf/BLGtKyFWOziWP+yA2ysSx34KCAsfwhCQubxZD0H6a4sLjiP2QqCSSLqMk0aYStGicw1NGImWuAm21/WKHCVKiEDhl0dFRTQKsRc917f5zPxWu97T233Htuez7PR3Jzvt/P9/P93s87nL7Oh8/5nnNTVUiS+vAbyz0ASdLoGPqS1BFDX5I6YuhLUkcMfUnqyMrlHsDhnHLKKbV27VoAfvKTn3D88ccv74CWgXX3pde6od/al6LunTt3/rCqXj/XsaM69NeuXcsDDzwAwNTUFJOTk8s7oGVg3X3ptW7ot/alqDvJfx/qmMs7ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkaP6E7kv1drNdw/V74kbL1nikUjS0cGZviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkeGCv0kf5nk4STfTfLFJK9IckaSHUkeS3J7kuNa35e3/d3t+NpZ17mutT+a5MKlKUmSdCjzhn6S1cAHgImq+l1gBXA5cBNwc1WtA54FrmqnXAU8W1VvBG5u/UhyZjvvzcB64NNJVixuOZKkwxl2eWcl8MokK4FXAXuB84A72/GtwKVte0Pbpx0/P0la+21V9UJVfR/YDZz90kuQJA1r3tCvqv8B/hZ4kkHYPw/sBJ6rqunWbQ+wum2vBp5q5063/ifPbp/jHEnSCMz7l7OSnMhgln4G8BzwT8BFc3StmVMOcexQ7Qf/vk3AJoBVq1YxNTUFwIEDB17cHta1Z03P3wkWfN1ROpK6x4F196fX2kdd9zB/LvGdwPer6gcASb4M/AFwQpKVbTa/Bni69d8DnA7sactBrwP2z2qfMfucF1XVFmALwMTERE1OTgKDYJ7ZHtaVw/65xCsWdt1ROpK6x4F196fX2kdd9zBr+k8C5yR5VVubPx94BLgPeE/rsxG4q21va/u04/dWVbX2y9vdPWcA64BvLk4ZkqRhzDvTr6odSe4EvgVMAw8ymInfDdyW5GOt7ZZ2yi3A55PsZjDDv7xd5+EkdzB4wZgGrq6qXyxyPZKkwxhmeYequh64/qDmx5nj7puq+hlw2SGucwNwwwLHKElaJH4iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4MFfpJTkhyZ5LvJdmV5PeTnJRke5LH2uOJrW+SfCrJ7iTfTvL2WdfZ2Po/lmTjUhUlSZrbsDP9TwL/UlW/A7wF2AVsBu6pqnXAPW0f4CJgXfvZBHwGIMlJwPXAO4CzgetnXigkSaMxb+gneS3wR8AtAFX186p6DtgAbG3dtgKXtu0NwOdq4BvACUlOAy4EtlfV/qp6FtgOrF/UaiRJh7VyiD5vAH4A/H2StwA7gWuAVVW1F6Cq9iY5tfVfDTw16/w9re1Q7b8iySYG/4fAqlWrmJqaAuDAgQMvbg/r2rOmh+q30OuO0pHUPQ6suz+91j7quocJ/ZXA24H3V9WOJJ/kl0s5c8kcbXWY9l9tqNoCbAGYmJioyclJYBDMM9vDunLz3UP1e+KKhV13lI6k7nFg3f3ptfZR1z3Mmv4eYE9V7Wj7dzJ4EXimLdvQHvfN6n/6rPPXAE8fpl2SNCLzhn5V/S/wVJLfbk3nA48A24CZO3A2Ane17W3Ae9tdPOcAz7dloK8DFyQ5sb2Be0FrkySNyDDLOwDvB76Q5DjgceB9DF4w7khyFfAkcFnr+zXgYmA38NPWl6ran+SjwP2t30eqav+iVCFJGspQoV9VDwETcxw6f46+BVx9iOvcCty6kAFKkhaPn8iVpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0ZOvSTrEjyYJKvtv0zkuxI8liS25Mc19pf3vZ3t+NrZ13jutb+aJILF7sYSdLhLWSmfw2wa9b+TcDNVbUOeBa4qrVfBTxbVW8Ebm79SHImcDnwZmA98OkkK17a8CVJCzFU6CdZA1wCfLbtBzgPuLN12Qpc2rY3tH3a8fNb/w3AbVX1QlV9H9gNnL0YRUiShrNyyH6fAD4EvKbtnww8V1XTbX8PsLptrwaeAqiq6STPt/6rgW/Muubsc16UZBOwCWDVqlVMTU0BcODAgRe3h3XtWdPzd4IFX3eUjqTucWDd/em19lHXPW/oJ3kXsK+qdiaZnGmeo2vNc+xw5/yyoWoLsAVgYmKiJicHv3JqaoqZ7WFdufnuofo9ccXCrjtKR1L3OLDu/vRa+6jrHmamfy7w7iQXA68AXstg5n9CkpVttr8GeLr13wOcDuxJshJ4HbB/VvuM2edIkkZg3jX9qrquqtZU1VoGb8TeW1VXAPcB72ndNgJ3te1tbZ92/N6qqtZ+ebu75wxgHfDNRatEkjSvYdf05/LXwG1JPgY8CNzS2m8BPp9kN4MZ/uUAVfVwkjuAR4Bp4Oqq+sVL+P2SpAVaUOhX1RQw1bYfZ467b6rqZ8Blhzj/BuCGhQ5SkrQ4/ESuJHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerIvKGf5PQk9yXZleThJNe09pOSbE/yWHs8sbUnyaeS7E7y7SRvn3Wtja3/Y0k2Ll1ZkqS5DDPTnwaurao3AecAVyc5E9gM3FNV64B72j7ARcC69rMJ+AwMXiSA64F3AGcD18+8UEiSRmPe0K+qvVX1rbb9Y2AXsBrYAGxt3bYCl7btDcDnauAbwAlJTgMuBLZX1f6qehbYDqxf1GokSYe1oDX9JGuBtwE7gFVVtRcGLwzAqa3bauCpWaftaW2HapckjcjKYTsmeTXwJeCDVfWjJIfsOkdbHab94N+zicGyEKtWrWJqagqAAwcOvLg9rGvPmh6q30KvO0pHUvc4sO7+9Fr7qOseKvSTvIxB4H+hqr7cmp9JclpV7W3LN/ta+x7g9FmnrwGebu2TB7VPHfy7qmoLsAVgYmKiJicHp0xNTTGzPawrN989VL8nrljYdUfpSOoeB9bdn15rH3Xdw9y9E+AWYFdVfXzWoW3AzB04G4G7ZrW/t93Fcw7wfFv++TpwQZIT2xu4F7Q2SdKIDDPTPxf4c+A7SR5qbR8GbgTuSHIV8CRwWTv2NeBiYDfwU+B9AFW1P8lHgftbv49U1f5FqUKSNJR5Q7+q/oO51+MBzp+jfwFXH+JatwK3LmSAkqTF4ydyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVk6K9WHmdrh/02zhsvWeKRSNLScqYvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyMrlHsCxZO3muxf1ek/ceMmiXk+S5uNMX5I6YuhLUkcMfUnqyMjX9JOsBz4JrAA+W1U3jnoMR4th3iO49qxpJpd+KJI6MdLQT7IC+DvgT4A9wP1JtlXVI6Mcx7hayBvNvoks9WnUM/2zgd1V9ThAktuADYChfxiLfdfQUl1zGL7YSMtr1KG/Gnhq1v4e4B2zOyTZBGxquweSPNq2TwF+uOQjPMp8YMzqzk1Ddx2ruheg17qh39qXou7fOtSBUYd+5mirX9mp2gJs+bUTkweqamKpBna0su6+9Fo39Fv7qOse9d07e4DTZ+2vAZ4e8RgkqVujDv37gXVJzkhyHHA5sG3EY5Ckbo10eaeqppP8BfB1Brds3lpVDw95+q8t+XTCuvvSa93Qb+0jrTtVNX8vSdJY8BO5ktQRQ1+SOnJMhH6S9UkeTbI7yeblHs9SSXJrkn1Jvjur7aQk25M81h5PXM4xLoUkpye5L8muJA8nuaa1j3XtSV6R5JtJ/rPV/Tet/YwkO1rdt7ebHsZOkhVJHkzy1bY/9nUneSLJd5I8lOSB1jbS5/lRH/qzvrrhIuBM4E+TnLm8o1oy/wCsP6htM3BPVa0D7mn742YauLaq3gScA1zd/huPe+0vAOdV1VuAtwLrk5wD3ATc3Op+FrhqGce4lK4Bds3a76XuP66qt866N3+kz/OjPvSZ9dUNVfVzYOarG8ZOVf0bsP+g5g3A1ra9Fbh0pIMagaraW1Xfats/ZhAEqxnz2mvgQNt9Wfsp4DzgztY+dnUDJFkDXAJ8tu2HDuo+hJE+z4+F0J/rqxtWL9NYlsOqqtoLg3AETl3m8SypJGuBtwE76KD2tsTxELAP2A78F/BcVU23LuP6fP8E8CHg/9r+yfRRdwH/mmRn+8oZGPHz/Fj4c4nzfnWDxkOSVwNfAj5YVT8aTP7GW1X9AnhrkhOArwBvmqvbaEe1tJK8C9hXVTuTTM40z9F1rOpuzq2qp5OcCmxP8r1RD+BYmOn3/tUNzyQ5DaA97lvm8SyJJC9jEPhfqKovt+YuageoqueAKQbvaZyQZGZCNo7P93OBdyd5gsFy7XkMZv7jXjdV9XR73MfgRf5sRvw8PxZCv/evbtgGbGzbG4G7lnEsS6Kt594C7Kqqj886NNa1J3l9m+GT5JXAOxm8n3Ef8J7WbezqrqrrqmpNVa1l8O/53qq6gjGvO8nxSV4zsw1cAHyXET/Pj4lP5Ca5mMFMYOarG25Y5iEtiSRfBCYZfNXqM8D1wD8DdwC/CTwJXFZVB7/Ze0xL8ofAvwPf4ZdrvB9msK4/trUn+T0Gb9ytYDABu6OqPpLkDQxmwCcBDwJ/VlUvLN9Il05b3vmrqnrXuNfd6vtK210J/GNV3ZDkZEb4PD8mQl+StDiOheUdSdIiMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR/4f/9f/0CJGAukAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.groupby('label_group').count()['image'].hist(bins=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2     6979\n",
       "3     1779\n",
       "4      862\n",
       "5      468\n",
       "6      282\n",
       "7      154\n",
       "8      118\n",
       "9       91\n",
       "10      48\n",
       "12      39\n",
       "11      38\n",
       "13      28\n",
       "14      19\n",
       "15      19\n",
       "16      13\n",
       "17       9\n",
       "51       7\n",
       "21       6\n",
       "20       6\n",
       "22       6\n",
       "Name: image, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby('label_group').count()['image'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the products have only a few offers (2-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are contained on the folder train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bf147de7e932b44b57d961854f2da122',\n",
       " '5679405e23c704bb4296505fdc1bdc8b',\n",
       " 'ca25d1085f7d90278d23299a8419fbe7',\n",
       " '6ed5195d22d560a037a298680ab948e3',\n",
       " '1e24285a491d9b5673d1d9c0e13650ac',\n",
       " 'c6824fbe5f7974a57fa2de0aea664927',\n",
       " 'a39063acff5167d307c7bf1a6022ff70',\n",
       " '7808dc5b8f1bce90d66cedf9dfab12ab',\n",
       " '45de21867494d0570d2962775331550e',\n",
       " '7741376b86dad8778948202889312155']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_path = 'data/train_images/'\n",
    "image_ids = [s.split('.')[0] for s in os.listdir(images_path)]\n",
    "image_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchvision\n",
    "import torch\n",
    "import random\n",
    "import functools\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use torchvision to load and process images. Each image is of 1024x1024 pixels which is too much so we resize it to 224x224 which is the imagenet standard. The embedding of the images will be done by the model so we just need to return the raw images. As for the text, right now we will simply use a TfidfVectorizer to embed titles, we will afterwards use pretrained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset) :\n",
    "    def __init__(self, images_path, df_path, tfs_image=torchvision.transforms.Resize((224, 224)), text_tokenizer=None) :\n",
    "        super(TripletDataset, self).__init__()\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        \n",
    "        # Reshaping only\n",
    "        # data augm + norm is done on batches for faster computation \n",
    "        self.tfs_image = tfs_image\n",
    "        \n",
    "        self.df = pd.read_csv(df_path)\n",
    "        self.df['label_group'] = self.df['label_group'].astype('category').cat.codes\n",
    "        self.df['index'] = range(self.df.shape[0])\n",
    "        self.labels = self.df['label_group'].unique()\n",
    "        self.label_to_index_list = self.df.groupby('label_group')['index'].apply(list)\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        index_meta = self.df.iloc[index]\n",
    "        \n",
    "        anchor_image, anchor_text = self._get_item(index)\n",
    "        \n",
    "        label = index_meta['label_group']\n",
    "        \n",
    "        # positive sample\n",
    "        pos_index = random.choice(self.label_to_index_list[label])\n",
    "        # we don't want the positive sample being the same as the anchor\n",
    "        while pos_index == index :\n",
    "            pos_index = random.choice(self.label_to_index_list[label])\n",
    "        pos_image, pos_text = self._get_item(pos_index)\n",
    "        \n",
    "        #negative sample\n",
    "        neg_label = random.choice(self.labels)\n",
    "        # Negative sample has to be different label from anchor \n",
    "        while neg_label == index :\n",
    "            neg_label = random.choice(self.labels)\n",
    "        neg_index = random.choice(self.label_to_index_list[neg_label])\n",
    "        neg_image, neg_text = self._get_item(neg_index)\n",
    "        \n",
    "        return anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text\n",
    "        \n",
    "    def _get_item(self, index) :\n",
    "        image = self.tfs_image(torchvision.io.read_image(os.path.join(self.images_path, self.df.iloc[index]['image'])))\n",
    "        text = self.df.iloc[index]['title']\n",
    "        return image, text\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(tokenizer, samples) :\n",
    "    batch_size = len(samples)\n",
    "    anchor_images, anchor_texts, pos_images, pos_texts, neg_images, neg_texts = zip(*samples)\n",
    "    anchor_images = torch.stack(anchor_images)\n",
    "    pos_images = torch.stack(pos_images)\n",
    "    neg_images = torch.stack(neg_images)\n",
    "    anchor_texts = tokenizer(list(anchor_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    pos_texts = tokenizer(list(pos_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    neg_texts = tokenizer(list(neg_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return anchor_images, anchor_texts, pos_images, pos_texts, neg_images, neg_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(images_path, df_paths, pretrianed_tokenizer='distilbert-base-uncased', batch_size=64) :\n",
    "    dataset = TripletDataset(images_path, df_paths)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrianed_tokenizer)\n",
    "    dl = DataLoader(dataset, batch_size=batch_size, collate_fn=partial(collate_fn, tokenizer))\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import timm\n",
    "from transformers import DistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbedorNN(nn.Module) :\n",
    "    def __init__(self, pretrained_image_embedor='resnet18', pretrained_text_embedor='distilbert-base-uncased',\n",
    "                output_dim=124, ) :\n",
    "        super(EmbedorNN, self).__init__()\n",
    "        self.image_embedor = timm.create_model(pretrained_image_embedor, pretrained=True)\n",
    "        self.text_embedor = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.head = nn.Linear(512+39, output_dim)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        images, texts = x\n",
    "        out_images = self.image_embedor.forward_features(images).mean(-1).mean(-1)\n",
    "        out_text = self.text_embedor(texts['input_ids'], attention_mask=texts['attention_mask']).get('last_hidden_state').mean(-1)\n",
    "        out = torch.cat([out_images, out_text], dim=-1)\n",
    "        return self.head(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
