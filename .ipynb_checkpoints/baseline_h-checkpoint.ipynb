{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "from functools import partial\n",
    "import PIL\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train val split\n",
    "labels = np.random.permutation(df['label_group'].unique())\n",
    "\n",
    "train_perc = 0.7\n",
    "train_idx = int(train_perc * len(labels))\n",
    "\n",
    "train_labels = labels[:train_idx]\n",
    "val_labels = labels[train_idx:]\n",
    "\n",
    "train_df = df[df['label_group'].isin(train_labels)]\n",
    "val_df = df[df['label_group'].isin(val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a756327777b2fe8e9383ab84468f3e5a',\n",
       " 'c58ef3697a5ae269fc16a6d3d4b56877',\n",
       " '05f1bc3c03271e5d4b5af7a7b263facf',\n",
       " '2826339257579760a00d6e0a0065a89e',\n",
       " '451b108436f28c18b2dc8bf63b712c08',\n",
       " '2f5c66fc9bb86bc81dd461cc6ad574e1',\n",
       " 'cbd375037243af186a4aa8d3aa08a489',\n",
       " '4edcfc239c17445eb949103c9f13eed3',\n",
       " '67e712eb6d5b08c0eb9e074cd7cc71c4',\n",
       " '712782879a3ef3acbb0c8eb945335528']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "images_path = 'data/train_images/'\n",
    "image_ids = [s.split('.')[0] for s in os.listdir(images_path)]\n",
    "image_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDataset(Dataset) :\n",
    "    def __init__(self, images_path, df, img_tfms, testing, text_tokenizer=None):\n",
    "        super(TripletDataset, self).__init__()\n",
    "        \n",
    "        self.images_path = images_path\n",
    "        self.img_tfms = img_tfms\n",
    "        self.testing = testing\n",
    "              \n",
    "        self.df = df.copy()\n",
    "        self.df['label_group'] = self.df['label_group'].astype('category').cat.codes\n",
    "        self.df['index'] = range(self.df.shape[0])\n",
    "        self.labels = self.df['label_group'].unique()\n",
    "        self.label_to_index_list = self.df.groupby('label_group')['index'].apply(list)\n",
    "        \n",
    "    def __getitem__(self, index) :\n",
    "        index_meta = self.df.iloc[index]\n",
    "        \n",
    "        anchor_image, anchor_text = self._get_item(index)\n",
    "        \n",
    "        if self.testing: return anchor_image, anchor_text\n",
    "        \n",
    "        label = index_meta['label_group']\n",
    "        \n",
    "        # positive sample\n",
    "        pos_index = random.choice(self.label_to_index_list[label])\n",
    "        # we don't want the positive sample being the same as the anchor\n",
    "        while pos_index == index :\n",
    "            pos_index = random.choice(self.label_to_index_list[label])\n",
    "        pos_image, pos_text = self._get_item(pos_index)\n",
    "        \n",
    "        #negative sample\n",
    "        neg_label = random.choice(self.labels)\n",
    "        # Negative sample has to be different label from anchor \n",
    "        while neg_label == index :\n",
    "            neg_label = random.choice(self.labels)\n",
    "        neg_index = random.choice(self.label_to_index_list[neg_label])\n",
    "        neg_image, neg_text = self._get_item(neg_index)\n",
    "        \n",
    "        return anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text\n",
    "        \n",
    "    def _get_item(self, index) :\n",
    "        image = PIL.Image.open(os.path.join(self.images_path, \n",
    "                                            self.df.iloc[index]['image']))\n",
    "        image = self.img_tfms(image)\n",
    "        text = self.df.iloc[index]['title']\n",
    "        return image, text\n",
    "    \n",
    "    def __len__(self) :\n",
    "        return self.df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(tokenizer, samples) :\n",
    "    batch_size = len(samples)\n",
    "    if len(samples[0]) == 2:\n",
    "        images, texts = zip(*samples)\n",
    "        images = torch.stack(images)\n",
    "        texts = tokenizer(list(texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        return images, texts\n",
    "    anchor_images, anchor_texts, pos_images, pos_texts, neg_images, neg_texts = zip(*samples)\n",
    "    anchor_images = torch.stack(anchor_images)\n",
    "    pos_images = torch.stack(pos_images)\n",
    "    neg_images = torch.stack(neg_images)\n",
    "    anchor_texts = tokenizer(list(anchor_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    pos_texts = tokenizer(list(pos_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    neg_texts = tokenizer(list(neg_texts), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return anchor_images, anchor_texts, pos_images, pos_texts, neg_images, neg_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dl(images_path, df_paths, img_tfms, pretrianed_tokenizer='distilbert-base-uncased', \n",
    "              batch_size=64, shuffle = True, testing = False) :\n",
    "    dataset = TripletDataset(images_path, df_paths, img_tfms, testing)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(pretrianed_tokenizer)\n",
    "    dl = DataLoader(dataset, batch_size=batch_size, collate_fn=partial(collate_fn, tokenizer), \n",
    "                    shuffle = shuffle, pin_memory = True)\n",
    "    return dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbedorNN(nn.Module) :\n",
    "    def __init__(self, pretrained_image_embedor='resnet18', pretrained_text_embedor='distilbert-base-uncased',\n",
    "                output_dim=128) :\n",
    "        super(EmbedorNN, self).__init__()\n",
    "        self.image_embedor = timm.create_model(pretrained_image_embedor, pretrained=True)\n",
    "        self.image_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.text_embedor = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "        self.head = nn.Linear(512+768, output_dim)\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        images, texts = x\n",
    "        out_images = self.image_embedor.forward_features(images)\n",
    "        out_images = self.image_pool(out_images).squeeze()\n",
    "        out_text = self.text_embedor(texts['input_ids'], \n",
    "                                     attention_mask=texts['attention_mask'])[0][:,0,:]\n",
    "        out = torch.cat([out_images, out_text], dim=-1)\n",
    "        return self.head(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((250, 250)),\n",
    "                                       transforms.ColorJitter(.25,.25,.25),\n",
    "                                       transforms.RandomRotation(5),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       normalize\n",
    "                                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                     transforms.ToTensor(),\n",
    "                                     normalize\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = EmbedorNN().to(device)\n",
    "# weights: https://drive.google.com/drive/folders/19BGTC53p5YIAakWeCZfNIsceMX7cjHgp?usp=sharing\n",
    "model.load_state_dict(torch.load('3ep.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dl = create_dl(images_path, train_df, train_transforms, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dl = create_dl(images_path, val_df, val_transforms, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dl = create_dl(images_path, df, val_transforms, shuffle = False, testing = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3\n",
    "swa_start = int(0.75*n_epochs)\n",
    "\n",
    "lf = nn.TripletMarginLoss()\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 1e-3\n",
    "no_decay = [\"bias\", \"BatchNorm2d.weight\", \"BatchNorm2d.bias\", \"LayerNorm.weight\", 'LayerNorm.bias']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": wd,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in  model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr =lr, pct_start = 0.3, #anneal_strategy = 'linear',\n",
    "                                            total_steps = int(n_epochs * len(tr_dl)))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87c44b0db65b4bb4b8c9a1f15567071e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3167deb50f15495ca47ae57918556f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1632eb6e454d4e53b1f3ddc67d022945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0: Train loss 0.064 - Val loss 0.023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8792b030a09844edbac44dc1f12a54c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acbd62bf4214aa0b246667221277b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Train loss 0.018 - Val loss 0.018\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0901597420194d4195c59e1aa7718e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/375 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afff355d5874467cbe67c32290135600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 2: Train loss 0.008 - Val loss 0.012\n"
     ]
    }
   ],
   "source": [
    "tr_losses = []\n",
    "val_losses = []\n",
    "for ep in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    pbar = tqdm(tr_dl)\n",
    "    for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "        \n",
    "        \n",
    "        anchor = anchor_image.to(device), {'input_ids' : anchor_text['input_ids'].to(device),\n",
    "                                           'attention_mask' : anchor_text['attention_mask'].to(device)}\n",
    "        \n",
    "        pos = pos_image.to(device), {'input_ids' : pos_text['input_ids'].to(device),\n",
    "                                     'attention_mask' : pos_text['attention_mask'].to(device)}\n",
    "        \n",
    "        neg = neg_image.to(device), {'input_ids' : neg_text['input_ids'].to(device),\n",
    "                                     'attention_mask' : neg_text['attention_mask'].to(device)}\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            anchor_emb = model(anchor)\n",
    "            pos_emb = model(pos)\n",
    "            neg_emb = model(neg)\n",
    "            loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        \n",
    "        tr_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {round(np.mean(tr_loss),3)}\")\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_dl)\n",
    "        for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "\n",
    "            anchor = anchor_image.to(device), {'input_ids' : anchor_text['input_ids'].to(device),\n",
    "                                               'attention_mask' : anchor_text['attention_mask'].to(device)}\n",
    "\n",
    "            pos = pos_image.to(device), {'input_ids' : pos_text['input_ids'].to(device),\n",
    "                                         'attention_mask' : pos_text['attention_mask'].to(device)}\n",
    "\n",
    "            neg = neg_image.to(device), {'input_ids' : neg_text['input_ids'].to(device),\n",
    "                                         'attention_mask' : neg_text['attention_mask'].to(device)}\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                \n",
    "                anchor_emb = model(anchor)\n",
    "                pos_emb = model(pos)\n",
    "                neg_emb = model(neg)\n",
    "                loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            pbar.set_description(f\"Val loss: {round(np.mean(val_loss),3)}\")\n",
    "            \n",
    "    tr_loss = round(np.mean(tr_loss),3)\n",
    "    val_loss = round(np.mean(val_loss),3)\n",
    "    tr_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    summary = f\"Ep {ep}: Train loss {tr_loss} - Val loss {val_loss}\"\n",
    "    print(summary) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '3ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91032c50ff3a40b3942b0411811cefbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/536 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(testing_dl)\n",
    "    for image, text in pbar:\n",
    "        x = image.to(device), {'input_ids' : text['input_ids'].to(device),\n",
    "                               'attention_mask' : text['attention_mask'].to(device)}\n",
    "        y = model(x)\n",
    "        embs.append(y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = torch.cat(embs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df = pd.DataFrame(embs.numpy())\n",
    "emb_cols = [f'emb_{i}' for i in embs_df.columns]\n",
    "embs_df.columns = emb_cols\n",
    "embs_df['cls'] = df['label_group']\n",
    "embs_df['cls'] = embs_df['cls'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>34240</th>\n",
       "      <th>34241</th>\n",
       "      <th>34242</th>\n",
       "      <th>34243</th>\n",
       "      <th>34244</th>\n",
       "      <th>34245</th>\n",
       "      <th>34246</th>\n",
       "      <th>34247</th>\n",
       "      <th>34248</th>\n",
       "      <th>34249</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>emb_0</th>\n",
       "      <td>0.500960</td>\n",
       "      <td>0.237719</td>\n",
       "      <td>0.050669</td>\n",
       "      <td>0.718747</td>\n",
       "      <td>-0.581001</td>\n",
       "      <td>-0.097971</td>\n",
       "      <td>1.183702</td>\n",
       "      <td>1.779962</td>\n",
       "      <td>1.124665</td>\n",
       "      <td>-0.972785</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.564198</td>\n",
       "      <td>0.842688</td>\n",
       "      <td>-0.393273</td>\n",
       "      <td>-0.365358</td>\n",
       "      <td>-0.153456</td>\n",
       "      <td>-0.877197</td>\n",
       "      <td>0.058753</td>\n",
       "      <td>-0.112191</td>\n",
       "      <td>0.031476</td>\n",
       "      <td>0.074278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_1</th>\n",
       "      <td>-1.301394</td>\n",
       "      <td>-1.292176</td>\n",
       "      <td>-0.944553</td>\n",
       "      <td>-1.888656</td>\n",
       "      <td>-0.108013</td>\n",
       "      <td>-0.747545</td>\n",
       "      <td>-1.263595</td>\n",
       "      <td>-1.211068</td>\n",
       "      <td>-0.682983</td>\n",
       "      <td>0.179870</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.576654</td>\n",
       "      <td>-0.905915</td>\n",
       "      <td>-0.449244</td>\n",
       "      <td>-0.466996</td>\n",
       "      <td>-0.804786</td>\n",
       "      <td>-0.308678</td>\n",
       "      <td>0.259296</td>\n",
       "      <td>-0.835542</td>\n",
       "      <td>-0.930508</td>\n",
       "      <td>-1.531453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_2</th>\n",
       "      <td>-0.226438</td>\n",
       "      <td>0.306056</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.311137</td>\n",
       "      <td>-0.544753</td>\n",
       "      <td>-0.013903</td>\n",
       "      <td>0.166399</td>\n",
       "      <td>0.306137</td>\n",
       "      <td>0.915265</td>\n",
       "      <td>-0.525376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455988</td>\n",
       "      <td>0.210848</td>\n",
       "      <td>-0.447949</td>\n",
       "      <td>-0.411966</td>\n",
       "      <td>0.376221</td>\n",
       "      <td>-0.085963</td>\n",
       "      <td>0.045716</td>\n",
       "      <td>0.809573</td>\n",
       "      <td>-0.265023</td>\n",
       "      <td>0.909191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_3</th>\n",
       "      <td>-0.323761</td>\n",
       "      <td>0.608568</td>\n",
       "      <td>-0.098363</td>\n",
       "      <td>-1.106508</td>\n",
       "      <td>0.221210</td>\n",
       "      <td>-1.320947</td>\n",
       "      <td>-0.539112</td>\n",
       "      <td>-1.217819</td>\n",
       "      <td>0.357320</td>\n",
       "      <td>-0.854563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597615</td>\n",
       "      <td>-0.977988</td>\n",
       "      <td>1.731781</td>\n",
       "      <td>1.720604</td>\n",
       "      <td>0.479843</td>\n",
       "      <td>0.006830</td>\n",
       "      <td>0.526009</td>\n",
       "      <td>-0.228569</td>\n",
       "      <td>0.531991</td>\n",
       "      <td>0.439010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_4</th>\n",
       "      <td>-1.258861</td>\n",
       "      <td>-0.940595</td>\n",
       "      <td>0.709853</td>\n",
       "      <td>-0.407680</td>\n",
       "      <td>0.241497</td>\n",
       "      <td>-1.016383</td>\n",
       "      <td>-0.178965</td>\n",
       "      <td>-0.405246</td>\n",
       "      <td>0.401834</td>\n",
       "      <td>-0.620351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.869939</td>\n",
       "      <td>-1.265399</td>\n",
       "      <td>-0.175623</td>\n",
       "      <td>-0.220973</td>\n",
       "      <td>-0.101543</td>\n",
       "      <td>-1.238433</td>\n",
       "      <td>-0.110854</td>\n",
       "      <td>-0.539160</td>\n",
       "      <td>0.548752</td>\n",
       "      <td>-1.179566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_124</th>\n",
       "      <td>1.327470</td>\n",
       "      <td>0.503480</td>\n",
       "      <td>0.680511</td>\n",
       "      <td>0.594233</td>\n",
       "      <td>-0.113412</td>\n",
       "      <td>-0.175654</td>\n",
       "      <td>0.622813</td>\n",
       "      <td>0.947124</td>\n",
       "      <td>0.934831</td>\n",
       "      <td>-0.190990</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.169104</td>\n",
       "      <td>0.302227</td>\n",
       "      <td>1.240302</td>\n",
       "      <td>1.217515</td>\n",
       "      <td>-0.716665</td>\n",
       "      <td>-0.396859</td>\n",
       "      <td>0.584340</td>\n",
       "      <td>-0.777377</td>\n",
       "      <td>0.648708</td>\n",
       "      <td>-0.493271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_125</th>\n",
       "      <td>-1.127460</td>\n",
       "      <td>-0.561340</td>\n",
       "      <td>0.595688</td>\n",
       "      <td>-0.708277</td>\n",
       "      <td>-0.253344</td>\n",
       "      <td>-1.247321</td>\n",
       "      <td>-0.849796</td>\n",
       "      <td>-1.481531</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>-1.126404</td>\n",
       "      <td>...</td>\n",
       "      <td>0.344415</td>\n",
       "      <td>-0.091109</td>\n",
       "      <td>1.479870</td>\n",
       "      <td>1.414771</td>\n",
       "      <td>-0.354530</td>\n",
       "      <td>0.216521</td>\n",
       "      <td>-0.839207</td>\n",
       "      <td>-1.230204</td>\n",
       "      <td>0.316971</td>\n",
       "      <td>-1.293721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_126</th>\n",
       "      <td>0.465477</td>\n",
       "      <td>-1.320146</td>\n",
       "      <td>0.288588</td>\n",
       "      <td>0.464027</td>\n",
       "      <td>0.089630</td>\n",
       "      <td>-0.426252</td>\n",
       "      <td>-0.943907</td>\n",
       "      <td>0.281021</td>\n",
       "      <td>0.681009</td>\n",
       "      <td>0.782818</td>\n",
       "      <td>...</td>\n",
       "      <td>1.783656</td>\n",
       "      <td>0.248377</td>\n",
       "      <td>0.240647</td>\n",
       "      <td>0.235544</td>\n",
       "      <td>0.519699</td>\n",
       "      <td>-0.012075</td>\n",
       "      <td>0.809881</td>\n",
       "      <td>1.609108</td>\n",
       "      <td>-0.025901</td>\n",
       "      <td>-0.320838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>emb_127</th>\n",
       "      <td>1.034373</td>\n",
       "      <td>-0.168213</td>\n",
       "      <td>-0.304716</td>\n",
       "      <td>-0.008191</td>\n",
       "      <td>0.086427</td>\n",
       "      <td>1.383384</td>\n",
       "      <td>0.928859</td>\n",
       "      <td>0.351101</td>\n",
       "      <td>0.364785</td>\n",
       "      <td>0.460414</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302482</td>\n",
       "      <td>-0.573974</td>\n",
       "      <td>0.280889</td>\n",
       "      <td>0.266239</td>\n",
       "      <td>-0.264183</td>\n",
       "      <td>-0.374306</td>\n",
       "      <td>1.208320</td>\n",
       "      <td>-0.497938</td>\n",
       "      <td>-0.462315</td>\n",
       "      <td>0.550078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cls</th>\n",
       "      <td>666.000000</td>\n",
       "      <td>7572.000000</td>\n",
       "      <td>6172.000000</td>\n",
       "      <td>10509.000000</td>\n",
       "      <td>9425.000000</td>\n",
       "      <td>6836.000000</td>\n",
       "      <td>4687.000000</td>\n",
       "      <td>3976.000000</td>\n",
       "      <td>6076.000000</td>\n",
       "      <td>6754.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>340.000000</td>\n",
       "      <td>8761.000000</td>\n",
       "      <td>9200.000000</td>\n",
       "      <td>9200.000000</td>\n",
       "      <td>7971.000000</td>\n",
       "      <td>9735.000000</td>\n",
       "      <td>7038.000000</td>\n",
       "      <td>10537.000000</td>\n",
       "      <td>4242.000000</td>\n",
       "      <td>1163.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 34250 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0            1            2             3            4      \\\n",
       "emb_0      0.500960     0.237719     0.050669      0.718747    -0.581001   \n",
       "emb_1     -1.301394    -1.292176    -0.944553     -1.888656    -0.108013   \n",
       "emb_2     -0.226438     0.306056     0.002641      0.311137    -0.544753   \n",
       "emb_3     -0.323761     0.608568    -0.098363     -1.106508     0.221210   \n",
       "emb_4     -1.258861    -0.940595     0.709853     -0.407680     0.241497   \n",
       "...             ...          ...          ...           ...          ...   \n",
       "emb_124    1.327470     0.503480     0.680511      0.594233    -0.113412   \n",
       "emb_125   -1.127460    -0.561340     0.595688     -0.708277    -0.253344   \n",
       "emb_126    0.465477    -1.320146     0.288588      0.464027     0.089630   \n",
       "emb_127    1.034373    -0.168213    -0.304716     -0.008191     0.086427   \n",
       "cls      666.000000  7572.000000  6172.000000  10509.000000  9425.000000   \n",
       "\n",
       "               5            6            7            8            9      ...  \\\n",
       "emb_0      -0.097971     1.183702     1.779962     1.124665    -0.972785  ...   \n",
       "emb_1      -0.747545    -1.263595    -1.211068    -0.682983     0.179870  ...   \n",
       "emb_2      -0.013903     0.166399     0.306137     0.915265    -0.525376  ...   \n",
       "emb_3      -1.320947    -0.539112    -1.217819     0.357320    -0.854563  ...   \n",
       "emb_4      -1.016383    -0.178965    -0.405246     0.401834    -0.620351  ...   \n",
       "...              ...          ...          ...          ...          ...  ...   \n",
       "emb_124    -0.175654     0.622813     0.947124     0.934831    -0.190990  ...   \n",
       "emb_125    -1.247321    -0.849796    -1.481531     0.011094    -1.126404  ...   \n",
       "emb_126    -0.426252    -0.943907     0.281021     0.681009     0.782818  ...   \n",
       "emb_127     1.383384     0.928859     0.351101     0.364785     0.460414  ...   \n",
       "cls      6836.000000  4687.000000  3976.000000  6076.000000  6754.000000  ...   \n",
       "\n",
       "              34240        34241        34242        34243        34244  \\\n",
       "emb_0     -1.564198     0.842688    -0.393273    -0.365358    -0.153456   \n",
       "emb_1     -0.576654    -0.905915    -0.449244    -0.466996    -0.804786   \n",
       "emb_2      0.455988     0.210848    -0.447949    -0.411966     0.376221   \n",
       "emb_3      0.597615    -0.977988     1.731781     1.720604     0.479843   \n",
       "emb_4     -0.869939    -1.265399    -0.175623    -0.220973    -0.101543   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "emb_124   -0.169104     0.302227     1.240302     1.217515    -0.716665   \n",
       "emb_125    0.344415    -0.091109     1.479870     1.414771    -0.354530   \n",
       "emb_126    1.783656     0.248377     0.240647     0.235544     0.519699   \n",
       "emb_127   -0.302482    -0.573974     0.280889     0.266239    -0.264183   \n",
       "cls      340.000000  8761.000000  9200.000000  9200.000000  7971.000000   \n",
       "\n",
       "               34245        34246         34247        34248        34249  \n",
       "emb_0      -0.877197     0.058753     -0.112191     0.031476     0.074278  \n",
       "emb_1      -0.308678     0.259296     -0.835542    -0.930508    -1.531453  \n",
       "emb_2      -0.085963     0.045716      0.809573    -0.265023     0.909191  \n",
       "emb_3       0.006830     0.526009     -0.228569     0.531991     0.439010  \n",
       "emb_4      -1.238433    -0.110854     -0.539160     0.548752    -1.179566  \n",
       "...              ...          ...           ...          ...          ...  \n",
       "emb_124    -0.396859     0.584340     -0.777377     0.648708    -0.493271  \n",
       "emb_125     0.216521    -0.839207     -1.230204     0.316971    -1.293721  \n",
       "emb_126    -0.012075     0.809881      1.609108    -0.025901    -0.320838  \n",
       "emb_127    -0.374306     1.208320     -0.497938    -0.462315     0.550078  \n",
       "cls      9735.000000  7038.000000  10537.000000  4242.000000  1163.000000  \n",
       "\n",
       "[129 rows x 34250 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df.to_csv('train_embs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
