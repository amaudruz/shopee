{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import functools\n",
    "from functools import partial\n",
    "import PIL\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import timm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating images of size (250, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_images_dir = 'data/small_train_images'\n",
    "\n",
    "create_small_train_imgs = False\n",
    "\n",
    "if not os.path.isdir(small_images_dir): \n",
    "    os.makedirs(small_images_dir)\n",
    "    create_small_train_imgs = True\n",
    "    \n",
    "if create_small_train_imgs:\n",
    "    big_img_path = 'data/train_images'\n",
    "    img_paths = pd.read_csv('data/train.csv')['image'].tolist()\n",
    "    sz = (250, 250)\n",
    "\n",
    "    for im_path in tqdm(img_paths):\n",
    "        im = PIL.Image.open(os.path.join(big_img_path, im_path))\n",
    "        im = im.resize(sz)\n",
    "        im.save(os.path.join(small_images_dir, im_path), quality = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletDS(Dataset):\n",
    "    def __init__(self, data, tokenizer, images_path, return_triplet = True):\n",
    "        super().__init__()\n",
    "        self.imgs = data['image'].tolist()\n",
    "        self.unique_labels = data['label_group'].unique().tolist()\n",
    "        self.labels = data['label_group'].tolist()\n",
    "        self.label_to_index_dict = (data.reset_index(drop = True)\n",
    "                                    .groupby('label_group')\n",
    "                                    .apply(lambda x: x.index.tolist())\n",
    "                                    .to_dict())\n",
    "        self.texts = tokenizer(data['title'].values.tolist(), return_tensors = 'pt',\n",
    "                               padding=True, truncation=True, max_length = 40)\n",
    "        self.images_path = images_path\n",
    "        self.return_triplet = return_triplet\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # anchor data\n",
    "        anchor_label = self.labels[idx]\n",
    "        anchor_img, anchor_txt = self._get_item(idx)\n",
    "        \n",
    "        if not self.return_triplet: return anchor_img, anchor_txt\n",
    "        \n",
    "        # neg data\n",
    "        neg_label = np.random.choice(self.unique_labels)\n",
    "        while neg_label == anchor_label:\n",
    "            neg_label = np.random.choice(self.unique_labels)\n",
    "        neg_idx = np.random.choice(self.label_to_index_dict[neg_label])\n",
    "        neg_img, neg_txt = self._get_item(neg_idx)   \n",
    "        \n",
    "        # pos data\n",
    "        pos_idxs = self.label_to_index_dict[anchor_label]\n",
    "        # picking an index not equal to anchor's index\n",
    "        pos_idxs = [o for o in pos_idxs if o != idx]\n",
    "        \n",
    "        if len(pos_idxs) == 0:\n",
    "            # edge case, only 1 sample per label\n",
    "            pos_idxs = [idx]\n",
    "        pos_idx = np.random.choice(pos_idxs)\n",
    "        pos_img, pos_txt = self._get_item(pos_idx)\n",
    "        \n",
    "        return anchor_img, anchor_txt, pos_img, pos_txt, neg_img, neg_txt\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def _get_item(self, idx):\n",
    "        im = PIL.Image.open(os.path.join(self.images_path, self.imgs[idx]))\n",
    "        im = torch.tensor(np.array(im) / 255.0, dtype = torch.float).permute(2,0,1)\n",
    "        txt = {'input_ids' : self.texts['input_ids'][idx], \n",
    "               'attention_mask' : self.texts['attention_mask'][idx]}\n",
    "        return im, txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in data\n",
    "\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "# train val split\n",
    "\n",
    "train_perc = 0.7\n",
    "n_train_examples = int(train_perc * len(df))\n",
    "\n",
    "train_df = df.iloc[:n_train_examples]\n",
    "val_df = df.iloc[n_train_examples:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "\n",
    "vision_model = 'resnet50'\n",
    "language_model = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(language_model)\n",
    "\n",
    "\n",
    "bs = 32\n",
    "tr_ds = TripletDS(train_df, tokenizer, small_images_dir)\n",
    "tr_dl = DataLoader(tr_ds, batch_size = bs, shuffle = True, pin_memory = True)\n",
    "\n",
    "val_ds = TripletDS(val_df, tokenizer, small_images_dir)\n",
    "val_dl = DataLoader(val_ds, batch_size = bs, shuffle = False, pin_memory = True)\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_device(text, device):\n",
    "    return {'input_ids' : text['input_ids'].to(device),\n",
    "            'attention_mask' : text['attention_mask'].to(device)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EmbedorNN(nn.Module) :\n",
    "    def __init__(self, pretrained_image_embedor='resnet50', pretrained_text_embedor='bert-base-uncased',\n",
    "                output_dim=512) :\n",
    "        super(EmbedorNN, self).__init__()\n",
    "        self.image_embedor = timm.create_model(pretrained_image_embedor, pretrained=True)\n",
    "        self.image_pool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.text_embedor = BertModel.from_pretrained(pretrained_text_embedor)\n",
    "        self.head = nn.Sequential(nn.Linear(2048+768, output_dim), \n",
    "                                  #nn.ReLU(), \n",
    "                                  #nn.Linear(1024, output_dim)\n",
    "                                 )\n",
    "        \n",
    "        for m in self.head.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                sz = m.weight.data.size(-1)\n",
    "                m.weight.data.normal_(mean=0.0, std=1/np.sqrt(sz))\n",
    "            elif isinstance(m, (nn.LayerNorm, nn.BatchNorm1d)):\n",
    "                m.bias.data.zero_()\n",
    "                m.weight.data.fill_(1.0)\n",
    "                m.bias.data.zero_()\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "            \n",
    "    def freeze_lm(self):\n",
    "        for parameter in self.text_embedor.parameters():\n",
    "            parameter.requires_grad = False\n",
    "            \n",
    "    def unfreeze_lm(self):\n",
    "        for parameter in self.text_embedor.parameters():\n",
    "            parameter.requires_grad = True\n",
    "            \n",
    "    def freeze_cnn(self):\n",
    "        for parameter in self.image_embedor.parameters():\n",
    "            parameter.requires_grad = False\n",
    "            \n",
    "    def unfreeze_cnn(self):\n",
    "        for parameter in self.image_embedor.parameters():\n",
    "            parameter.requires_grad = True\n",
    "    \n",
    "    def forward(self, x) :\n",
    "        images, texts = x\n",
    "        out_images = self.image_embedor.forward_features(images)\n",
    "        out_images = self.image_pool(out_images).squeeze()\n",
    "        out_text = self.text_embedor(texts['input_ids'], \n",
    "                                     attention_mask=texts['attention_mask'])[0][:,0,:]\n",
    "        out = torch.cat([out_images, out_text], dim=-1)\n",
    "        return F.normalize(self.head(out), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model instantiation\n",
    "\n",
    "model = EmbedorNN(vision_model, language_model).to(device)\n",
    "\n",
    "model.freeze_lm()\n",
    "model.freeze_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training params\n",
    "\n",
    "normalize = transforms.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                                 std=(0.229, 0.224, 0.225))\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.ColorJitter(.3,.3,.3),\n",
    "                                       transforms.RandomRotation(5),\n",
    "                                       transforms.RandomCrop(224),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       normalize\n",
    "                                       ])\n",
    "\n",
    "val_transforms = transforms.Compose([transforms.Resize((224,224)),\n",
    "                                     normalize\n",
    "                                     ])\n",
    "\n",
    "n_epochs = 2\n",
    "\n",
    "lf = nn.TripletMarginLoss()\n",
    "\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "no_decay = [\"bias\", \"BatchNorm2d.weight\", \"BatchNorm2d.bias\", \"LayerNorm.weight\", 'LayerNorm.bias',\n",
    "            \"BatchNorm1d.weight\", \"BatchNorm1d.bias\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": wd,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in  model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr =lr, pct_start = 0.3, #anneal_strategy = 'linear',\n",
    "                                            total_steps = int(n_epochs * len(tr_dl)))\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee4a1099504437794887213f665061c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580b6ea8ca714c12b6a5e1ac3de99b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca3896d321f4b999c51a1560e86d3d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 0: Train loss 0.434 - Val loss 0.177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "510e7cb2d74f4f4da5ccd0f6916ccbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b864b0ac434a89b9647f89b5d5c7a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/322 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep 1: Train loss 0.278 - Val loss 0.169\n"
     ]
    }
   ],
   "source": [
    "tr_losses = []\n",
    "val_losses = []\n",
    "for ep in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    pbar = tqdm(tr_dl)\n",
    "    for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "        \n",
    "        anchor = train_transforms(anchor_image.to(device)), text_to_device(anchor_text, device)\n",
    "        pos = train_transforms(pos_image.to(device)), text_to_device(pos_text, device)\n",
    "        neg = train_transforms(neg_image.to(device)), text_to_device(neg_text, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            anchor_emb = model(anchor)\n",
    "            pos_emb = model(pos)\n",
    "            neg_emb = model(neg)\n",
    "            loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        \n",
    "        tr_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {round(np.mean(tr_loss),3)}\")\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_dl)\n",
    "        for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "\n",
    "            anchor = val_transforms(anchor_image.to(device)), text_to_device(anchor_text, device)\n",
    "            pos = val_transforms(pos_image.to(device)), text_to_device(pos_text, device)\n",
    "            neg = val_transforms(neg_image.to(device)), text_to_device(neg_text, device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                \n",
    "                anchor_emb = model(anchor)\n",
    "                pos_emb = model(pos)\n",
    "                neg_emb = model(neg)\n",
    "                loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            pbar.set_description(f\"Val loss: {round(np.mean(val_loss),3)}\")\n",
    "            \n",
    "    tr_loss = round(np.mean(tr_loss),3)\n",
    "    val_loss = round(np.mean(val_loss),3)\n",
    "    if ep >= 5:\n",
    "        if val_loss < val_losses[-1]:\n",
    "            torch.save(model.state_dict(), f'loss_{ep}_{val_loss}.pt')\n",
    "    tr_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    summary = f\"Ep {ep}: Train loss {tr_loss} - Val loss {val_loss}\"\n",
    "    print(summary) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.unfreeze_lm()\n",
    "model.unfreeze_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '2ep_frozen.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('2ep_frozen.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3f76a13302b4b67b077778b8ba79f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4933d7af36f941ffb05928e1b01b08c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/750 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "swa_start = int(0.75*n_epochs)\n",
    "\n",
    "lr = 2e-5\n",
    "wd = 1e-4\n",
    "no_decay = [\"bias\", \"BatchNorm2d.weight\", \"BatchNorm2d.bias\", \"LayerNorm.weight\", 'LayerNorm.bias',\n",
    "            \"BatchNorm1d.weight\", \"BatchNorm1d.bias\"]\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": wd,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in  model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=lr)\n",
    "\n",
    "# learning rate scheduler\n",
    "sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr =lr, pct_start = 0.3, #anneal_strategy = 'linear',\n",
    "                                            total_steps = int(n_epochs * len(tr_dl)))\n",
    "\n",
    "tr_losses = []\n",
    "val_losses = []\n",
    "for ep in tqdm(range(n_epochs)):\n",
    "    model.train()\n",
    "    tr_loss = []\n",
    "    pbar = tqdm(tr_dl)\n",
    "    for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "        \n",
    "        anchor = train_transforms(anchor_image.to(device)), text_to_device(anchor_text, device)\n",
    "        pos = train_transforms(pos_image.to(device)), text_to_device(pos_text, device)\n",
    "        neg = train_transforms(neg_image.to(device)), text_to_device(neg_text, device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            anchor_emb = model(anchor)\n",
    "            pos_emb = model(pos)\n",
    "            neg_emb = model(neg)\n",
    "            loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        \n",
    "        tr_loss.append(loss.item())\n",
    "        pbar.set_description(f\"Train loss: {round(np.mean(tr_loss),3)}\")\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(val_dl)\n",
    "        for anchor_image, anchor_text, pos_image, pos_text, neg_image, neg_text in pbar:\n",
    "\n",
    "            anchor = val_transforms(anchor_image.to(device)), text_to_device(anchor_text, device)\n",
    "            pos = val_transforms(pos_image.to(device)), text_to_device(pos_text, device)\n",
    "            neg = val_transforms(neg_image.to(device)), text_to_device(neg_text, device)\n",
    "\n",
    "            with torch.cuda.amp.autocast():\n",
    "                \n",
    "                anchor_emb = model(anchor)\n",
    "                pos_emb = model(pos)\n",
    "                neg_emb = model(neg)\n",
    "                loss = lf(anchor_emb, pos_emb, neg_emb)\n",
    "\n",
    "            val_loss.append(loss.item())\n",
    "            pbar.set_description(f\"Val loss: {round(np.mean(val_loss),3)}\")\n",
    "            \n",
    "    tr_loss = round(np.mean(tr_loss),3)\n",
    "    val_loss = round(np.mean(val_loss),3)\n",
    "    if ep >= 5:\n",
    "        if val_loss < val_losses[-1]:\n",
    "            torch.save(model.state_dict(), f'loss_{ep}_{val_loss}.pt')\n",
    "    tr_losses.append(tr_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    summary = f\"Ep {ep}: Train loss {tr_loss} - Val loss {val_loss}\"\n",
    "    print(summary) \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_ds = TripletDS(df, tokenizer, small_images_dir)\n",
    "testing_dl = DataLoader(testing_ds, batch_size = bs, shuffle = False, pin_memory = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5826d2fbd51a4a33800101900b015d2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    pbar = tqdm(testing_dl)\n",
    "    for image, text in pbar:\n",
    "        x = image.to(device), {'input_ids' : text['input_ids'].to(device),\n",
    "                               'attention_mask' : text['attention_mask'].to(device)}\n",
    "        y = model(x)\n",
    "        embs.append(y.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = torch.cat(embs,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df = pd.DataFrame(embs.numpy())\n",
    "emb_cols = [f'emb_{i}' for i in embs_df.columns]\n",
    "embs_df.columns = emb_cols\n",
    "embs_df.to_csv('train_embs.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_df.to_csv('train_embs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
